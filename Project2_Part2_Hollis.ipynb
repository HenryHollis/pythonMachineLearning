{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2_Part2_Hollis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNkJh7X37rqxnTeFnJZkll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenryHollis/pythonMachineLearning/blob/master/Project2_Part2_Hollis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sRmBD6cb6uQ",
        "colab_type": "text"
      },
      "source": [
        "# Option 3: ANN With a Hidden Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjYeTV5qWCnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All my imports\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import keras  #for MNIST data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG-PYrAhWEG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bring in my MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data(path=\"mnist.npz\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1uPVO_lAPvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "517893f3-8101-4b80-f9c6-51ad0ef63c6a"
      },
      "source": [
        "# Reshape the training and test examples\n",
        "y_train = y_train.reshape((60000, 1))\n",
        "y_test = y_test.reshape((10000, 1))\n",
        "X_train= X_train.reshape(X_train.shape[0], -1).T\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
        "\n",
        "print (\"X_train shape: \" + str(X_train.shape))  # This is the format Dr. Ng suggests\n",
        "print (\"y_train shape: \" + str(y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"y_test shape: \" + str(y_test.shape))\n",
        "\n",
        "X_train = X_train/255.   # normalize px values\n",
        "X_test = X_test/255."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (784, 60000)\n",
            "y_train shape: (60000, 1)\n",
            "X_test shape: (784, 10000)\n",
            "y_test shape: (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfhUh7fhIh3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making categorical data:\n",
        "def one_hot_encode(labels):\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc = enc.fit_transform(labels).toarray()\n",
        "    return enc.T\n",
        "\n",
        "# Rectified Linear Unit (ReLU) activation function\n",
        "def Relu(Z):\n",
        "    return np.maximum(0,Z)  \n",
        "\n",
        "# derivative of ReLU\n",
        "def dRelu(x):\n",
        "    x[x<=0] = 0  \n",
        "    x[x>0] = 1\n",
        "    return x\n",
        "\n",
        "#Remember, Relu has a piecewise deriv.\n",
        "def dRelu2(dZ, Z):    \n",
        "    dZ[Z <= 0] = 0    \n",
        "    return dZ\n",
        "\n",
        "# Defining my activation function for output layer\n",
        "def softmax(x):  \n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "# derivative of softmax (also sigmoid)\n",
        "def d_softmax(Z):\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = s * (1-s)\n",
        "    return dZ"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0VbuLbGV2yB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class digit_classifier: \n",
        "    \"\"\"Here I modified some code I have from my AI class last semester:\n",
        "     https://github.com/HenryHollis/pythonMachineLearning/blob/master/shroom/Classifying%20Mushrooms.ipynb\n",
        "     \"\"\"\n",
        "    def __init__(self, X_train, y_train, learn_rate = 0.003):\n",
        "        \"\"\" input: X_train: ndarray of shape (numfeatures, numsamples)\n",
        "                   y_train: ndarray of shape (1, numsamples)\n",
        "                   learning_rate: float val\n",
        "        \"\"\"\n",
        "        #print(y_train.shape)\n",
        "        self.X = X_train                           # training input vector size (784, 60000)\n",
        "        self.Y = one_hot_encode(y_train)           # training output vector size (1, 60000)\n",
        "        y_train = y_train.T\n",
        "        #print('X shape', self.X.shape)\n",
        "        #print('y shape', self.Y.shape)\n",
        "        self.Yh = np.zeros(self.Y.shape[1]).reshape((1, -1))   # actual outputs of the NN \n",
        "        self.L = 2                            # number of layers in our NN\n",
        "        self.dims = [784, 500, 10]            # dimensions of layers: 784 inputs, 500 units (hidden), 1 output unit\n",
        "        self.param = {}  \n",
        "        self.ch = {}                          # dictionary to store intermediate results of forward pass for backprop use\n",
        "        self.grad = {}\n",
        "        self.loss = []\n",
        "        self.train_acc = []\n",
        "        self.lr = learn_rate                  # learning rate\n",
        "        self.sam = self.Y.shape[1]            # number of training samples to use\n",
        "       \n",
        "    # initialize the network weights with random values instead of zeros like Prof. Ng suggests\n",
        "    def initialize(self):    \n",
        "        np.random.seed(1)\n",
        "        self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) \n",
        "        self.param['b1'] = np.zeros((self.dims[1], 1))        \n",
        "        self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) \n",
        "        self.param['b2'] = np.zeros((self.dims[2], 1))          \n",
        "        # print(self.param['W1'].shape)      \n",
        "        # print(self.param['b1'].shape)      \n",
        "        # print(self.param['W2'].shape)      \n",
        "        # print(self.param['b2'].shape)      \n",
        "\n",
        "        return\n",
        "        \n",
        "    # forward pass of the NN\n",
        "    def propagate(self, predict = False):\n",
        "        \"\"\"\n",
        "        When predicting we need to use this method but we\n",
        "        dont care about the loss, hence the parameter \"predict\"\n",
        "        \"\"\"\n",
        "        Z1 = self.param['W1'].dot(self.X) + self.param['b1'] # score is calculated z1 = W dot X + B\n",
        "        A1 = Relu(Z1)                                        # This is the preactivation\n",
        "        self.ch['Z1'],self.ch['A1']=Z1, A1                   # store these results for later backprop\n",
        "        Z2 = self.param['W2'].dot(A1) + self.param['b2']     # preactivation 2 calculated\n",
        "        A2 = softmax(Z2)                                     # run through softmax function\n",
        "        self.ch['Z2'],self.ch['A2']=Z2,A2                    # again, store intermediate results of hidden layer\n",
        "        self.Yh = A2                                         # final outputs stored in Yh\n",
        "        loss = np.NAN\n",
        "        if not predict:\n",
        "            loss = self.nloss(A2)                            # calculate loss on final output, then return\n",
        "        return self.Yh, loss\n",
        "        \n",
        "\n",
        "    # compute the Loss\n",
        "    def nloss(self,Yh):\n",
        "      \"\"\"\n",
        "      loss function\n",
        "      input: ndarry of shape: (numClasses, numSamples)\n",
        "      \"\"\"\n",
        "      loss = (1.0/self.sam) * (-np.dot(self.Y, np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))    \n",
        "      return loss   \n",
        "    \n",
        "    # backward pass\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        backprop using chain rule and\n",
        "        derivative functions we defined\n",
        "        \"\"\"\n",
        "        dLoss_Yh = - (np.divide(self.Y, self.Yh ) - np.divide(1 - self.Y, 1 - self.Yh))  # derivative of loss function\n",
        "        #print(self.ch['Z2'].shape)\n",
        "        dLoss_Z2 = dLoss_Yh * d_softmax(self.ch['Z2'])    # derivative of loss w.r.t. preactivation 2\n",
        "        dLoss_A1 = np.dot(self.param[\"W2\"].T,dLoss_Z2)   # dloss w.r.t. activation 1\n",
        "\n",
        "        # using chain rule to find dLoss w.r.t weights and bias for layer 2\n",
        "        dLoss_W2 = 1.0/self.ch['A1'].shape[1] * np.dot(dLoss_Z2,self.ch['A1'].T)\n",
        "        dLoss_b2 = 1.0/self.ch['A1'].shape[1] * np.dot(dLoss_Z2, np.ones([dLoss_Z2.shape[1],1])) \n",
        "                            \n",
        "        dLoss_Z1 = dLoss_A1 * dRelu(self.ch['Z1'])        \n",
        "        dLoss_A0 = np.dot(self.param[\"W1\"].T,dLoss_Z1)\n",
        "\n",
        "        # using chain rule to find dLoss w.r.t weights and bias for layer 1\n",
        "        dLoss_W1 = 1./self.X.shape[1] * np.dot(dLoss_Z1,self.X.T)\n",
        "        dLoss_b1 = 1./self.X.shape[1] * np.dot(dLoss_Z1, np.ones([dLoss_Z1.shape[1],1]))  \n",
        "        \n",
        "        # Adjust the weights and bias accordingly\n",
        "        self.param[\"W1\"] -= self.lr * dLoss_W1   # adjust weights between input and hidden\n",
        "        self.param[\"b1\"] -= self.lr * dLoss_b1\n",
        "        self.param[\"W2\"] -= self.lr * dLoss_W2   # adjust weights between hidden and output\n",
        "        self.param[\"b2\"] -= self.lr * dLoss_b2 \n",
        "\n",
        "    def predict(self, X_test, y_test = []): \n",
        "        \"\"\"\n",
        "        This method has two modes:\n",
        "        When test labels are supplied via \"y_test\" \n",
        "        it returns validation acc.\n",
        "        When just test data is supplied it outputs the predictions\n",
        "        and confidence.\n",
        "        \"\"\"\n",
        "        self.X = X_test                           # training input vector size (784, 10000)\n",
        "        if len(y_test) > 0:\n",
        "          self.Y = one_hot_encode(y_test)           # training output vector size (1, 10000)\n",
        "          y_test = y_test.T\n",
        "          #print('X shape', self.X.shape)\n",
        "          #print('y shape', self.Y.shape)\n",
        "          self.Yh = np.zeros(self.Y.shape[1]).reshape((1, -1))   # actual outputs of the NN \n",
        "          pred, loss = self.propagate()\n",
        "          pred = softmax(pred)\n",
        "          pred = pred.argmax(axis=0)\n",
        "          acc = np.mean(pred == np.squeeze(y_test))\n",
        "          print('Validation Acc: {:.4f}'.format(acc))\n",
        "          return pred\n",
        "        else:\n",
        "          self.Yh = np.array(X_test[1]).reshape((1, -1))\n",
        "          #print(self.Yh)\n",
        "          pred,_ = self.propagate(True)\n",
        "          #print(pred)\n",
        "          conf = np.amax(pred, axis=0)\n",
        "          conf = [ '%.2f' % x for x in conf ]\n",
        "          conf = [ float(x) for x in conf ]\n",
        "          pred = pred.argmax(axis=0)\n",
        "          preds_conf = (list(zip(list(pred), list(conf))))\n",
        "          print(\"Prediction Complete (prediction, confidence):\")\n",
        "        return (preds_conf)\n",
        "    \n",
        "    def fit(self,X, Y, epoch = 2000):\n",
        "      \"\"\" model for training. Used \"fit\"\n",
        "           to align with keras convention.\n",
        "           input: X : ndarray: (numfeatures, numsamples)\n",
        "                  Y : ndarray: (1, numsamples)\n",
        "                  epoch: How many times do we run through all samples\n",
        "           returns: train accuracy and loss\n",
        "      \"\"\"\n",
        "      np.random.seed(1)\n",
        "      self.initialize()\n",
        "      iter = []         #I use these lists for graphing the progress later\n",
        "      cost = []\n",
        "      for i in range(0, epoch):\n",
        "            Yh, loss = self.propagate()  # forward pass\n",
        "            self.backward()              # backprop\n",
        "            \n",
        "            if i % 10 == 0:\n",
        "                iter.append(i)\n",
        "                cost.append(np.mean(loss))\n",
        "                print ('Cost after iteration {}: {:.4f}'.format(i, np.mean(loss)))\n",
        "                self.loss.append(loss)\n",
        "                preds = Yh.argmax(axis=0)             \n",
        "                acc = np.mean(preds == np.squeeze(y_train))\n",
        "                print('Training Acc: {:.4f}'.format(acc))\n",
        "                self.train_acc.append(acc)\n",
        "      plt.plot(iter, cost)\n",
        "      #print(iter, cost)\n",
        "      plt.ylabel('cost')\n",
        "      plt.xlabel('iterations')\n",
        "      plt.title('Cost as a Function of Epoch')\n",
        "      plt.show()\n",
        "    \n",
        "      return acc, np.mean(loss)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu0Rzo5UWBTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = digit_classifier(X_train, y_train, learn_rate=0.06)  # create new classifier object"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXIjOn4iWzE0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b263158-f68f-4e20-c6a7-1e58b2522eb8"
      },
      "source": [
        "model.fit(X_train, y_train, epoch = 400)  # train model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.3283\n",
            "Training Acc: 0.1033\n",
            "Cost after iteration 10: 0.3399\n",
            "Training Acc: 0.7691\n",
            "Cost after iteration 20: 0.3661\n",
            "Training Acc: 0.7798\n",
            "Cost after iteration 30: 0.3964\n",
            "Training Acc: 0.7766\n",
            "Cost after iteration 40: 0.4287\n",
            "Training Acc: 0.7782\n",
            "Cost after iteration 50: 0.4613\n",
            "Training Acc: 0.7843\n",
            "Cost after iteration 60: 0.4935\n",
            "Training Acc: 0.7924\n",
            "Cost after iteration 70: 0.5247\n",
            "Training Acc: 0.8003\n",
            "Cost after iteration 80: 0.5547\n",
            "Training Acc: 0.8070\n",
            "Cost after iteration 90: 0.5833\n",
            "Training Acc: 0.8131\n",
            "Cost after iteration 100: 0.6104\n",
            "Training Acc: 0.8193\n",
            "Cost after iteration 110: 0.6361\n",
            "Training Acc: 0.8246\n",
            "Cost after iteration 120: 0.6603\n",
            "Training Acc: 0.8297\n",
            "Cost after iteration 130: 0.6833\n",
            "Training Acc: 0.8341\n",
            "Cost after iteration 140: 0.7050\n",
            "Training Acc: 0.8382\n",
            "Cost after iteration 150: 0.7255\n",
            "Training Acc: 0.8427\n",
            "Cost after iteration 160: 0.7450\n",
            "Training Acc: 0.8463\n",
            "Cost after iteration 170: 0.7635\n",
            "Training Acc: 0.8499\n",
            "Cost after iteration 180: 0.7811\n",
            "Training Acc: 0.8530\n",
            "Cost after iteration 190: 0.7979\n",
            "Training Acc: 0.8560\n",
            "Cost after iteration 200: 0.8139\n",
            "Training Acc: 0.8589\n",
            "Cost after iteration 210: 0.8292\n",
            "Training Acc: 0.8614\n",
            "Cost after iteration 220: 0.8438\n",
            "Training Acc: 0.8636\n",
            "Cost after iteration 230: 0.8579\n",
            "Training Acc: 0.8656\n",
            "Cost after iteration 240: 0.8713\n",
            "Training Acc: 0.8675\n",
            "Cost after iteration 250: 0.8843\n",
            "Training Acc: 0.8691\n",
            "Cost after iteration 260: 0.8967\n",
            "Training Acc: 0.8712\n",
            "Cost after iteration 270: 0.9087\n",
            "Training Acc: 0.8729\n",
            "Cost after iteration 280: 0.9203\n",
            "Training Acc: 0.8742\n",
            "Cost after iteration 290: 0.9314\n",
            "Training Acc: 0.8756\n",
            "Cost after iteration 300: 0.9421\n",
            "Training Acc: 0.8766\n",
            "Cost after iteration 310: 0.9525\n",
            "Training Acc: 0.8778\n",
            "Cost after iteration 320: 0.9626\n",
            "Training Acc: 0.8789\n",
            "Cost after iteration 330: 0.9723\n",
            "Training Acc: 0.8801\n",
            "Cost after iteration 340: 0.9818\n",
            "Training Acc: 0.8809\n",
            "Cost after iteration 350: 0.9909\n",
            "Training Acc: 0.8817\n",
            "Cost after iteration 360: 0.9998\n",
            "Training Acc: 0.8825\n",
            "Cost after iteration 370: 1.0085\n",
            "Training Acc: 0.8833\n",
            "Cost after iteration 380: 1.0169\n",
            "Training Acc: 0.8843\n",
            "Cost after iteration 390: 1.0252\n",
            "Training Acc: 0.8850\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8849833333333333, 1.0324280549194351)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJkwPFuj6dwS",
        "colab_type": "text"
      },
      "source": [
        "I have tried various learning rates and this one seems to give the best training accuracy. Oddly enough, the cost rises as a function of epoch even though the training accuracy increases. At first I though this was overfitting but the validation accuracy in the next section shows that is not the case. I find this very curious..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncQKDMSsgIKW",
        "colab_type": "text"
      },
      "source": [
        "#Running Model on Unseen Test Set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUIP8a_uzh96",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "c85f2e3e-ef92-4740-8a0b-dfaf1beb0803"
      },
      "source": [
        "preds = model.predict(X_test, y_test)  # The model has never seen this test data\n",
        "\n",
        "confusion_matrix(np.squeeze(y_test.T), preds, labels=[0,1,2,3,4,5,6,7,8,9], sample_weight=None, normalize=None)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Acc: 0.8943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 943,    0,    3,    3,    0,    6,   19,    2,    4,    0],\n",
              "       [   0, 1114,    3,    4,    0,    2,    4,    0,    8,    0],\n",
              "       [  10,   14,  891,   24,   15,    1,   22,   17,   31,    7],\n",
              "       [   5,    2,   23,  907,    1,   27,    3,   16,   12,   14],\n",
              "       [   1,    6,    5,    0,  891,    1,   17,    1,    6,   54],\n",
              "       [  15,    3,    8,   44,   20,  742,   17,   10,   20,   13],\n",
              "       [  12,    3,   12,    1,   10,   17,  898,    2,    3,    0],\n",
              "       [   2,   25,   26,    4,   12,    0,    1,  916,    3,   39],\n",
              "       [  10,   12,   17,   49,   19,   43,   18,   14,  766,   26],\n",
              "       [  10,    8,    5,   11,   49,   17,    3,   25,    6,  875]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snKN70p0gThm",
        "colab_type": "text"
      },
      "source": [
        "# Running Unlabeled Data Through the Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8FrOcdzdfnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "d342c097-4ae0-4321-b0e8-a12233ef78ac"
      },
      "source": [
        "first10 = X_test[:, 0:10].reshape((784,10))  \n",
        "\n",
        "# Running first10 test through without labels makes the model return predicitions and confidence\n",
        "model.predict(first10)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction Complete (prediction, confidence):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7, 1.0),\n",
              " (2, 0.93),\n",
              " (1, 0.99),\n",
              " (0, 1.0),\n",
              " (4, 0.94),\n",
              " (1, 0.99),\n",
              " (4, 0.92),\n",
              " (9, 0.92),\n",
              " (6, 0.57),\n",
              " (9, 0.8)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFZ22SwohHGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}